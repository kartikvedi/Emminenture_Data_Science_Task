{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e2bc082-cf0c-41d5-be4d-74a26e363c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text saved to: output/Cleaned_Extracted_Text.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = [line.strip() for line in lines if line.strip()]  # Remove empty lines and strip spaces\n",
    "    return \" \".join(cleaned_lines)  # Join lines into a single paragraph for better readability\n",
    "\n",
    "# Path to the extracted PDF file\n",
    "pdf_path = \"/Users/kartikvedi/Desktop/Assignment/QA - Supporting Files/QA - 4 - PDF/_000011888-0.2.pdf\"\n",
    "\n",
    "# Dictionary to store structured text\n",
    "structured_text = {}\n",
    "\n",
    "# Extract text from pages 13 to 24\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    total_pages = len(pdf.pages)  # Get actual total pages\n",
    "    for page_num in range(13, min(25, total_pages + 1)):  # Ensure we don't exceed available pages\n",
    "        page = pdf.pages[page_num - 1]\n",
    "        structured_text[page_num] = page.extract_text()\n",
    "\n",
    "# Apply text cleaning and structure it for Excel\n",
    "cleaned_text_data = {\"Page Number\": [], \"Cleaned Text\": []}\n",
    "\n",
    "for page_num, text in structured_text.items():\n",
    "    cleaned_text_data[\"Page Number\"].append(page_num)\n",
    "    cleaned_text_data[\"Cleaned Text\"].append(clean_text(text))\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a DataFrame with cleaned text and save to Excel\n",
    "cleaned_text_df = pd.DataFrame(cleaned_text_data)\n",
    "cleaned_text_excel_path = os.path.join(output_dir, \"Cleaned_Extracted_Text.xlsx\")\n",
    "cleaned_text_df.to_excel(cleaned_text_excel_path, index=False)\n",
    "\n",
    "print(f\"Cleaned text saved to: {cleaned_text_excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82ebc721-54be-48b2-96dc-bd98e7b81e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page source for debugging: gray_point_debug.html\n",
      "Found 3 listings on Gray Point.\n",
      "Extracted: Sovereign Gate, 18-20 Kew Road, Richmond TW9 2NA, OFFICE TO LET RICHMOND / SERVICED OFFICE DESKS, Desk rates from £485 – £500 per desk\n",
      "Extracted: 179 High Street, Hampton Hill TW12 1NL, NOW LET TO ESTABLISHED SUPERMARKET CHAIN, N/A\n",
      "Extracted: 28 York Street, Twickenham TW1 3LJ, ﻿, N/A\n",
      "Saved page source for debugging: rightmove_debug.html\n",
      "Switched to an iframe.\n",
      "No properties found on Rightmove. Error: Message: Unable to locate frame for element: [object HTMLIFrameElement]\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:197:5\n",
      "NoSuchFrameError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:592:5\n",
      "switchToFrame@chrome://remote/content/marionette/actors/MarionetteCommandsChild.sys.mjs:707:15\n",
      "receiveMessage@chrome://remote/content/marionette/actors/MarionetteCommandsChild.sys.mjs:290:31\n",
      "\n",
      "Scraping completed. Data saved to GrayPoint_Properties.xlsx and RightMove_Properties.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" Remove unwanted HTML tags and extra spaces from extracted text \"\"\"\n",
    "    return re.sub(r'<[^>]+>', '', text).strip()\n",
    "\n",
    "def scroll_down(driver, times=5):\n",
    "    \"\"\" Scroll down multiple times to load all content \"\"\"\n",
    "    for _ in range(times):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "\n",
    "def debug_page_source(driver, filename):\n",
    "    \"\"\" Save and print page source for debugging \"\"\"\n",
    "    page_source = driver.page_source\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(page_source)\n",
    "    print(f\"Saved page source for debugging: {filename}\")\n",
    "\n",
    "def scrape_gray_point():\n",
    "    url = \"https://www.gray-point.com/properties/\"\n",
    "    driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()))\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    debug_page_source(driver, \"gray_point_debug.html\")  # Save page source for debugging\n",
    "    \n",
    "    properties = []\n",
    "    \n",
    "    scroll_down(driver)  # Ensure all properties are loaded\n",
    "    \n",
    "    try:\n",
    "        listings = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//div[contains(@class, 'PropertyContent')]\"))\n",
    "        )\n",
    "        print(f\"Found {len(listings)} listings on Gray Point.\")\n",
    "        \n",
    "        for listing in listings:\n",
    "            try:\n",
    "                name = clean_text(listing.find_element(By.XPATH, \".//h1\").text)\n",
    "            except:\n",
    "                name = \"N/A\"\n",
    "            try:\n",
    "                address = clean_text(listing.find_element(By.XPATH, \".//p\").text)\n",
    "            except:\n",
    "                address = \"N/A\"\n",
    "            try:\n",
    "                features = clean_text(listing.find_element(By.XPATH, \".//ul\").text)\n",
    "            except:\n",
    "                features = \"N/A\"\n",
    "            try:\n",
    "                price = clean_text(listing.find_element(By.XPATH, \".//p[contains(text(),'£')]\").text)\n",
    "            except:\n",
    "                price = \"N/A\"\n",
    "            \n",
    "            print(f\"Extracted: {name}, {address}, {price}\")\n",
    "            properties.append([name, address, features, price])\n",
    "    except Exception as e:\n",
    "        print(f\"No properties found on Gray Point. Error: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return properties\n",
    "\n",
    "def scrape_rightmove():\n",
    "    url = \"https://www.rightmove.co.uk/commercial-property-to-let.html\"\n",
    "    driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()))\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    debug_page_source(driver, \"rightmove_debug.html\")  # Save page source for debugging\n",
    "    \n",
    "    properties = []\n",
    "    \n",
    "    scroll_down(driver)  # Ensure all listings load\n",
    "    \n",
    "    try:\n",
    "        # Wait for iframes to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.TAG_NAME, \"iframe\"))\n",
    "        )\n",
    "        iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "        \n",
    "        for iframe in iframes:\n",
    "            driver.switch_to.frame(iframe)\n",
    "            print(\"Switched to an iframe.\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            try:\n",
    "                listings = driver.find_elements(By.XPATH, \"//div[contains(@class, 'propertyCard-wrapper')]\")\n",
    "                if listings:\n",
    "                    break  # Stop switching if listings are found\n",
    "            except:\n",
    "                driver.switch_to.default_content()  # Reset if no listings found\n",
    "        \n",
    "        driver.switch_to.default_content()  # Ensure we are back in main frame\n",
    "        \n",
    "        listings = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//div[contains(@class, 'propertyCard-wrapper')]\"))\n",
    "        )\n",
    "        print(f\"Found {len(listings)} listings on Rightmove.\")\n",
    "        \n",
    "        for listing in listings:\n",
    "            try:\n",
    "                name = clean_text(listing.find_element(By.XPATH, \".//h2\").text)\n",
    "            except:\n",
    "                name = \"N/A\"\n",
    "            try:\n",
    "                address = clean_text(listing.find_element(By.XPATH, \".//div[contains(@class, 'propertyCard-address')]\").text)\n",
    "            except:\n",
    "                address = \"N/A\"\n",
    "            try:\n",
    "                price = clean_text(listing.find_element(By.XPATH, \".//span[contains(@class, 'propertyCard-priceValue')]\" ).text)\n",
    "            except:\n",
    "                price = \"N/A\"\n",
    "            \n",
    "            print(f\"Extracted: {name}, {address}, {price}\")\n",
    "            properties.append([name, address, \"N/A\", price])  # Features unavailable\n",
    "    except Exception as e:\n",
    "        print(f\"No properties found on Rightmove. Error: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return properties\n",
    "\n",
    "def save_to_excel(data, filename):\n",
    "    df = pd.DataFrame(data, columns=[\"Name\", \"Address\", \"Features\", \"Price\"])\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gray_point_data = scrape_gray_point()\n",
    "    rightmove_data = scrape_rightmove()\n",
    "    \n",
    "    save_to_excel(gray_point_data, \"GrayPoint_Properties.xlsx\")\n",
    "    save_to_excel(rightmove_data, \"RightMove_Properties.xlsx\")\n",
    "    \n",
    "    print(\"Scraping completed. Data saved to GrayPoint_Properties.xlsx and RightMove_Properties.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bda08b35-3a3c-408a-8018-b2b6a64a561a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "response body:\n{\"message\":\"API rate limit exceeded for 146.196.35.178. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)\",\"documentation_url\":\"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"}\n\nrequest url:\nhttps://api.github.com/repos/mozilla/geckodriver/releases/latest\nresponse headers:\n{'Date': 'Mon, 24 Mar 2025 22:23:58 GMT', 'Server': 'Varnish', 'Strict-Transport-Security': 'max-age=31536000; includeSubdomains; preload', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'deny', 'X-XSS-Protection': '1; mode=block', 'Content-Security-Policy': \"default-src 'none'; style-src 'unsafe-inline'\", 'Access-Control-Allow-Origin': '*', 'Access-Control-Expose-Headers': 'ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset, X-RateLimit-Used, X-RateLimit-Resource, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset', 'Content-Type': 'application/json; charset=utf-8', 'Referrer-Policy': 'origin-when-cross-origin, strict-origin-when-cross-origin', 'X-GitHub-Media-Type': 'github.v3; format=json', 'X-RateLimit-Limit': '60', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1742855878', 'X-RateLimit-Resource': 'core', 'X-RateLimit-Used': '60', 'Content-Length': '280', 'X-GitHub-Request-Id': 'F06C:2966E8:FD4D:1BB29:67E1DB7E'}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     73\u001b[0m     profile_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.instagram.com/kartikaaryan/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 74\u001b[0m     scraped_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_instagram_posts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     save_to_excel(scraped_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstagram_Posts.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping completed. Data saved to Instagram_Posts.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m, in \u001b[0;36mscrape_instagram_posts\u001b[0;34m(profile_url, post_count)\u001b[0m\n\u001b[1;32m     12\u001b[0m options \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mFirefoxOptions()\n\u001b[1;32m     13\u001b[0m options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--headless\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Run in headless mode to reduce detection\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mFirefox(service\u001b[38;5;241m=\u001b[39mService(\u001b[43mGeckoDriverManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m), options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m     15\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(profile_url)\n\u001b[1;32m     16\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/webdriver_manager/firefox.py:39\u001b[0m, in \u001b[0;36mGeckoDriverManager.install\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minstall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 39\u001b[0m     driver_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_driver_binary_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     os\u001b[38;5;241m.\u001b[39mchmod(driver_path, \u001b[38;5;241m0o755\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m driver_path\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/webdriver_manager/core/manager.py:35\u001b[0m, in \u001b[0;36mDriverManager._get_driver_binary_path\u001b[0;34m(self, driver)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_driver_binary_path\u001b[39m(\u001b[38;5;28mself\u001b[39m, driver):\n\u001b[0;32m---> 35\u001b[0m     binary_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary_path:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/webdriver_manager/core/driver_cache.py:110\u001b[0m, in \u001b[0;36mDriverCacheManager.find_driver\u001b[0;34m(self, driver)\u001b[0m\n\u001b[1;32m    107\u001b[0m driver_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_cache_key_driver_version(driver)\n\u001b[1;32m    108\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_metadata_content()\n\u001b[0;32m--> 110\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_metadata_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata:\n\u001b[1;32m    112\u001b[0m     log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThere is no [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdriver_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdriver_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for browser \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrowser_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrowser_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in cache\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/webdriver_manager/core/driver_cache.py:144\u001b[0m, in \u001b[0;36mDriverCacheManager.__get_metadata_key\u001b[0;34m(self, driver)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_key:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_key\n\u001b[0;32m--> 144\u001b[0m driver_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cache_key_driver_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m browser_version \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mget_browser_version_from_os()\n\u001b[1;32m    146\u001b[0m browser_version \u001b[38;5;241m=\u001b[39m browser_version \u001b[38;5;28;01mif\u001b[39;00m browser_version \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/webdriver_manager/core/driver_cache.py:154\u001b[0m, in \u001b[0;36mDriverCacheManager.get_cache_key_driver_version\u001b[0;34m(self, driver)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_key_driver_version:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_key_driver_version\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_driver_version_to_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/webdriver_manager/core/driver.py:48\u001b[0m, in \u001b[0;36mDriver.get_driver_version_to_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver_version_to_download:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver_version_to_download\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_latest_release_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/webdriver_manager/drivers/firefox.py:29\u001b[0m, in \u001b[0;36mGeckoDriver.get_latest_release_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m determined_browser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_browser_version_from_os()\n\u001b[1;32m     28\u001b[0m log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet LATEST \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m version for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetermined_browser_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m firefox\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_http_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_release_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth_header\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtag_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/webdriver_manager/core/http.py:36\u001b[0m, in \u001b[0;36mWDMHttpClient.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mConnectionError:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mConnectionError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not reach host. Are you offline?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/webdriver_manager/core/http.py:19\u001b[0m, in \u001b[0;36mHttpClient.validate_response\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI Rate limit exceeded. You have to add GH_TOKEN!!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse body:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest url:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse headers:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdict\u001b[39m(resp\u001b[38;5;241m.\u001b[39mheaders)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: response body:\n{\"message\":\"API rate limit exceeded for 146.196.35.178. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)\",\"documentation_url\":\"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"}\n\nrequest url:\nhttps://api.github.com/repos/mozilla/geckodriver/releases/latest\nresponse headers:\n{'Date': 'Mon, 24 Mar 2025 22:23:58 GMT', 'Server': 'Varnish', 'Strict-Transport-Security': 'max-age=31536000; includeSubdomains; preload', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'deny', 'X-XSS-Protection': '1; mode=block', 'Content-Security-Policy': \"default-src 'none'; style-src 'unsafe-inline'\", 'Access-Control-Allow-Origin': '*', 'Access-Control-Expose-Headers': 'ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset, X-RateLimit-Used, X-RateLimit-Resource, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset', 'Content-Type': 'application/json; charset=utf-8', 'Referrer-Policy': 'origin-when-cross-origin, strict-origin-when-cross-origin', 'X-GitHub-Media-Type': 'github.v3; format=json', 'X-RateLimit-Limit': '60', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1742855878', 'X-RateLimit-Resource': 'core', 'X-RateLimit-Used': '60', 'Content-Length': '280', 'X-GitHub-Request-Id': 'F06C:2966E8:FD4D:1BB29:67E1DB7E'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "def scrape_instagram_posts(profile_url, post_count=5):\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode to reduce detection\n",
    "    driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()), options=options)\n",
    "    driver.get(profile_url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        # Wait until posts appear\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//article//a\"))\n",
    "        )\n",
    "        \n",
    "        # Find all post links\n",
    "        post_links = driver.find_elements(By.XPATH, \"//article//a\")[:post_count]\n",
    "        \n",
    "        for link in post_links:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", link)  # Scroll to element\n",
    "            time.sleep(2)\n",
    "            driver.execute_script(\"arguments[0].click();\", link)  # Use JavaScript click\n",
    "            time.sleep(5)  # Give time for content to load\n",
    "            \n",
    "            try:\n",
    "                caption = driver.find_element(By.XPATH, \"//div[@data-testid='post-caption']\").text\n",
    "            except:\n",
    "                try:\n",
    "                    caption = driver.find_element(By.XPATH, \"//meta[@property='og:description']\").get_attribute(\"content\")\n",
    "                except:\n",
    "                    caption = \"N/A\"\n",
    "            \n",
    "            try:\n",
    "                likes = driver.find_element(By.XPATH, \"//section[contains(@class, 'EDfFK ygqzn')]//span\").text\n",
    "            except:\n",
    "                likes = \"N/A\"\n",
    "            \n",
    "            try:\n",
    "                comments_section = driver.find_elements(By.XPATH, \"//ul[contains(@class, 'Mr508')]/div/li/div/div\")\n",
    "                comments = [comment.text for comment in comments_section]\n",
    "                comments = \" | \".join(comments) if comments else \"N/A\"\n",
    "            except:\n",
    "                comments = \"N/A\"\n",
    "            \n",
    "            print(f\"Extracted: {caption}, {likes}, {comments}\")\n",
    "            posts.append([profile_url, caption, likes, comments])\n",
    "            \n",
    "            driver.back()\n",
    "            time.sleep(2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping Instagram: {traceback.format_exc()}\")  # Capture full error details\n",
    "    \n",
    "    driver.quit()\n",
    "    return posts\n",
    "\n",
    "# Save extracted data to Excel\n",
    "def save_to_excel(data, filename):\n",
    "    df = pd.DataFrame(data, columns=[\"Profile URL\", \"Caption\", \"Likes\", \"Comments\"])\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    profile_url = \"https://www.instagram.com/kartikaaryan/\"\n",
    "    scraped_data = scrape_instagram_posts(profile_url, post_count=5)\n",
    "    save_to_excel(scraped_data, \"Instagram_Posts.xlsx\")\n",
    "    print(\"Scraping completed. Data saved to Instagram_Posts.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83354ec7-71b1-4c6f-a0dc-751c6bc42847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aadhaar details saved to /Users/kartikvedi/Desktop/Aadhaar_Details.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Updated path to Aadhaar images folder\n",
    "aadhaar_folder_path = \"/Users/kartikvedi/Desktop/Assignment/QA - Supporting Files/QA - 5 - Aadhar Cards\"\n",
    "\n",
    "# Path to save the cleaned Aadhaar details Excel file\n",
    "output_excel_path = \"/Users/kartikvedi/Desktop/Aadhaar_Details.xlsx\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes extra spaces, new lines, and unwanted characters from OCR output.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s,.-]', '', text).strip()\n",
    "\n",
    "def extract_aadhaar_details(text):\n",
    "    \"\"\"Extract Name, DOB, Gender, Aadhaar Number, and Address from OCR text\"\"\"\n",
    "    text = clean_text(text)\n",
    "    name_match = re.search(r\"Name\\s*[:]?\\s*([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*)\", text)\n",
    "    dob_match = re.search(r\"DOB[:\\s-]+(\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})\", text)\n",
    "    gender_match = re.search(r\"\\b(Male|Female|Other)\\b\", text, re.IGNORECASE)\n",
    "    aadhaar_match = re.search(r\"(\\d{4}\\s\\d{4}\\s\\d{4})\", text)\n",
    "    address_match = re.search(r\"Address[:]?\\s*(.*)\", text, re.DOTALL)\n",
    "\n",
    "    return {\n",
    "        \"Name\": name_match.group(1) if name_match else \"N/A\",\n",
    "        \"DOB\": dob_match.group(1) if dob_match else \"N/A\",\n",
    "        \"Gender\": gender_match.group(1).capitalize() if gender_match else \"N/A\",\n",
    "        \"Aadhaar Number\": aadhaar_match.group(1) if aadhaar_match else \"N/A\",\n",
    "        \"Address\": clean_text(address_match.group(1)) if address_match else \"N/A\",\n",
    "    }\n",
    "\n",
    "def process_aadhaar_images(folder_path):\n",
    "    \"\"\"Extract Aadhaar details from images in the specified folder\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Error: Aadhaar folder not found - {folder_path}\")\n",
    "        return []\n",
    "    \n",
    "    aadhaar_records = []\n",
    "    \n",
    "    for file in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, file)\n",
    "        if not os.path.isfile(image_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            extracted_text = pytesseract.image_to_string(image)\n",
    "            aadhaar_data = extract_aadhaar_details(extracted_text)\n",
    "            aadhaar_records.append(aadhaar_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    return aadhaar_records\n",
    "\n",
    "def save_to_excel(data, filename):\n",
    "    \"\"\"Save extracted data to an Excel file on Desktop\"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(output_excel_path, index=False)\n",
    "    print(f\"Aadhaar details saved to {output_excel_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extracted_data = process_aadhaar_images(aadhaar_folder_path)\n",
    "    if extracted_data:\n",
    "        save_to_excel(extracted_data, output_excel_path)\n",
    "    else:\n",
    "        print(\"No Aadhaar details extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8235e1-2a6f-42a8-9025-c1b11b61d2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
